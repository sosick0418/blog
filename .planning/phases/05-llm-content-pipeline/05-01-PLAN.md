---
phase: 05-llm-content-pipeline
plan: 01
type: execute
depends_on: []
files_modified: [src/_data/llm/config.js, src/_data/llm/gemini.js, src/_data/llm/index.js, .env.example]
---

<objective>
Set up Gemini API client module for LLM content generation.

Purpose: Establish the LLM integration foundation that the prompt engineering and post generation will build upon.
Output: Working Gemini API client module with configuration, ready for content generation.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-llm-content-pipeline/DISCOVERY.md

**Prior decisions:**
- ES modules for all data files (04-01)
- Environment variables for credentials (04-01)

**Tech stack available:**
- Node.js with ES modules
- 11ty data file system
- dotenv for environment variables
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM configuration module</name>
  <files>src/_data/llm/config.js</files>
  <action>
Create ES module configuration for LLM providers:

- `PROVIDER` - defaults to 'gemini' from LLM_PROVIDER env var
- `MODEL` - defaults to 'gemini-2.5-flash' from LLM_MODEL env var
- `API_KEY` - from GEMINI_API_KEY env var
- `MAX_RETRIES` - default 3
- `RETRY_DELAY_MS` - default 1000
- `isConfigured()` - returns true if API key is set
- `getProviderConfig()` - returns provider-specific settings

Use process.env directly (11ty loads .env automatically via dotenv).
Export as ES module (import/export syntax).
  </action>
  <verify>node -e "import('./src/_data/llm/config.js').then(m => console.log(m.isConfigured()))"</verify>
  <done>config.js exports provider config, isConfigured returns boolean based on env var presence</done>
</task>

<task type="auto">
  <name>Task 2: Create Gemini API client</name>
  <files>src/_data/llm/gemini.js</files>
  <action>
Create Gemini API client using @google/generative-ai npm package:

1. Install dependency: npm install @google/generative-ai

2. Create gemini.js module:
- Import GoogleGenerativeAI from @google/generative-ai
- Import config from ./config.js
- `initClient()` - Initialize Gemini client with API key
- `generateContent(prompt, options)` - Main generation function
  - options: { maxOutputTokens, temperature }
  - Returns generated text string
  - Implements retry logic with exponential backoff
- `generateProductReview(productData, systemPrompt)` - Convenience wrapper
  - Accepts product object and system prompt
  - Returns markdown content string

Error handling:
- Throw descriptive errors for missing API key
- Retry on rate limit (429) with exponential backoff
- Log retries to console

Do NOT include prompt templates here - that's Plan 05-02.
  </action>
  <verify>Test with: GEMINI_API_KEY=test node -e "import('./src/_data/llm/gemini.js').then(m => console.log('Module loaded'))"</verify>
  <done>gemini.js exports generateContent and generateProductReview functions, handles retries</done>
</task>

<task type="auto">
  <name>Task 3: Create main LLM index module and update .env.example</name>
  <files>src/_data/llm/index.js, .env.example</files>
  <action>
Create index.js as the main entry point:
- Re-export from config.js: isConfigured, getProviderConfig
- Re-export from gemini.js: generateContent, generateProductReview
- Add simple `testConnection()` function that makes minimal API call

Update .env.example with LLM configuration section:
```
# LLM Configuration (Phase 5)
GEMINI_API_KEY=           # Google AI Studio API key (https://aistudio.google.com/apikey)
LLM_PROVIDER=gemini       # LLM provider (gemini)
LLM_MODEL=gemini-2.5-flash # Model to use
```

Place after Coupang configuration section.
  </action>
  <verify>node -e "import('./src/_data/llm/index.js').then(m => console.log(Object.keys(m)))"</verify>
  <done>index.js re-exports all LLM functions, .env.example includes LLM configuration</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `src/_data/llm/` directory exists with config.js, gemini.js, index.js
- [ ] npm install @google/generative-ai completed without errors
- [ ] All modules import without errors
- [ ] .env.example contains GEMINI_API_KEY placeholder
- [ ] `npm run build` succeeds
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- LLM module structure ready for prompt engineering (Plan 05-02)
</success_criteria>

<output>
After completion, create `.planning/phases/05-llm-content-pipeline/05-01-SUMMARY.md`
</output>
